{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05-DQN_Surreal.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:gdg_denver] *",
      "language": "python",
      "name": "conda-env-gdg_denver-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMUn36UnEUo3"
      },
      "source": [
        "#Surreal Adversarial Learning, used here as simply an altered training paradigm that slowly increases a self-sabotoging\n",
        "#attack on the reward function, primarily here experimenting with how this type of targeted attack can increase both \n",
        "#adversarial robustness as well as overall performance via implicit self-supervised learning. \n",
        "\n",
        "#Uses a baseline DQN experiment from an author I cannot find, (apologies to the original author...)\n",
        "\n",
        "#Video series on Surreal Adversarial Learning: https://www.youtube.com/watch?v=gpIJ41IZj_o&list=PLKgB8_r-NpR5YcI1knBLHh7XZeh3ESKvn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0QXrblNnoxJ"
      },
      "source": [
        "# Deep Q-Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGWIMv2xnoxL"
      },
      "source": [
        "In 2015, Google DeepMind ([Link](https://deepmind.com/research/dqn/)) published a paper in Nature magazine that combines a deep convolution neural network with reinforcement learning for the first time in order to master a range of Atari 2600 games. They used only the raw pixels and score as the inputs. They were able to use the convolution layer to translate the pixels.  \n",
        "\n",
        "The very simple description is that they replaced the Q table in a Q-Learner with a neural network. This allowed them to take advantage of neural networks but still use reinforcement learning. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkuYw-SAnoxM",
        "outputId": "db8f6ea9-a358-4e7d-f978-b644bd7634d0"
      },
      "source": [
        "#Imports\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "#from keras.models import Sequential\n",
        "#from keras.layers import Dense\n",
        "#from keras.optimizers import Adam\n",
        "import random\n",
        "\n",
        "#Create Gym\n",
        "from gym import wrappers\n",
        "envCartPole = gym.make('CartPole-v1')\n",
        "envCartPole.seed(50) #Set the seed to keep the environment consistent across runs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[50]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcyNeXrBnoxS"
      },
      "source": [
        "**Experience Replay**  \n",
        "Definition: A mechanism inspired by biology that randomizes over the data removing the correlation in the observation sequence and smoothing over changes in the data distribution.  \n",
        "\n",
        "To perform an experience replay, the algorithm stores all of the agents experiences {$s_t,a_t,r_t,s_{t+1}$} at each time step in a data set. Normally in a q-learner, we would run the update rule on them. But, with experience replay we just store them.  \n",
        "\n",
        "Later during the training process these replays will be drawn uniformly from the memory queue and be ran through the update rule. There are 2 ways to handle this and I have coded both in the past. The first is to run them on every loop and the other is to run them after X amount of runs. In this code below, I run them each time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nANwa8Vlnoxh"
      },
      "source": [
        "**CartPole Example**  \n",
        "Again we will use the [CartPole](https://gym.openai.com/envs/CartPole-v1/) environment from OpenAI.  \n",
        "\n",
        "The actions are 0 to push the cart to the left and 1 to push the cart to the right.  \n",
        "\n",
        "The continuous state space is an X coordinate for location, the velocity of the cart, the angle of the pole, and the velocity at the tip of the pole. The X coordinate goes from -4.8 to +4.8, velocity is -Inf to +Inf, angle of the pole goes from -24 degrees to +24 degrees, tip velocity is -Inf to +Inf. With all of the possible combinations you can see why we can't create a Q table for each one.  \n",
        "\n",
        "To \"solve\" this puzzle you have to have an average reward of > 195 over 100 consecutive episodes. One thing to note, I am hard capping the rewards at 210 so this number can't average above that and it also could potentially drive the average down."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb8N4zdMnoxi"
      },
      "source": [
        "#Global Variables\n",
        "EPISODES = 500\n",
        "TRAIN_END = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5hK66Z6noxm"
      },
      "source": [
        "#Hyper Parameters\n",
        "#AKA your fiddly bits :)\n",
        "def discount_rate(): #Gamma\n",
        "    return 0.95\n",
        "\n",
        "def learning_rate(): #Alpha\n",
        "    return 0.001\n",
        "\n",
        "def batch_size(): #Size of the batch used in the experience replay\n",
        "    return 24"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "411ea0qInoxq"
      },
      "source": [
        "## **Deep Q-Network Class**  \n",
        "The following class is the deep Q-network that is built using the neural network code from Keras.  \n",
        "**init**:  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This creates the class and sets the local parameters.  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;I use a *deque* for the local memory to hold the experiences and a keras model for the NN.  \n",
        "\n",
        "**build_model(self)**:  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This builds the NN. I am using sequential model. Each of the layers are *Dense* despite the fact the document talks about using *Convolution*. But, they are only using that because they need to convert pixels and I already have numbers.  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;I am using an input layer(4), 24 neuron layer, 24 neuron layer, and an output layer(2).  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For calculating the loss I am using mean squared error.  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For an optimizer I am using [Adam](https://arxiv.org/abs/1412.6980v8). It is a variant of gradient descent and you can read the technical document at the link. If you want a slightly lighter explaining you can check out [Machine Learning Mastery](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/). You could also use SGD (Stochastic Gradient Descent) but Adam gives me better results and seems to be the standard in most examples.  \n",
        "\n",
        "**action(self,state)**:  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This generates the action.  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Explore: I am using the epsilon like previous lessons.  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Exploit: I use the NN to grab the 2 possible actions and then grab the argmax to find the better one  \n",
        "\n",
        "**test_action(self,state)**:  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This generates the action when I am testing. I want to 100% exploit  \n",
        "\n",
        "**store(self, state, action, reward, nstate, done)**:  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This places the observables in memory  \n",
        "\n",
        "**experience_replay(self, batch_size)**:  \n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This is where the training occurs. We grab the sample batches and then use the NN to predict the optimal action.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxZDoKBdnoxq"
      },
      "source": [
        "class DeepQNetwork():\n",
        "    def __init__(self, states, actions, alpha, gamma, epsilon,epsilon_min, epsilon_decay):\n",
        "        self.nS = states\n",
        "        self.nA = actions\n",
        "        self.memory = deque([], maxlen=2500)\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        #Explore/Exploit\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.model = self.build_model()\n",
        "        self.loss = []\n",
        "        \n",
        "    def build_model(self):\n",
        "        model = keras.Sequential() #linear stack of layers https://keras.io/models/sequential/\n",
        "        model.add(keras.layers.Dense(24, input_dim=self.nS, activation='relu')) #[Input] -> Layer 1\n",
        "        #   Dense: Densely connected layer https://keras.io/layers/core/\n",
        "        #   24: Number of neurons\n",
        "        #   input_dim: Number of input variables\n",
        "        #   activation: Rectified Linear Unit (relu) ranges >= 0\n",
        "        model.add(keras.layers.Dense(24, activation='relu')) #Layer 2 -> 3\n",
        "        model.add(keras.layers.Dense(self.nA, activation='linear')) #Layer 3 -> [output]\n",
        "        #   Size has to match the output (different actions)\n",
        "        #   Linear activation on the last layer\n",
        "        model.compile(loss='mean_squared_error', #Loss function: Mean Squared Error\n",
        "                      optimizer=keras.optimizers.Adam(lr=self.alpha)) #Optimaizer: Adam (Feel free to check other options)\n",
        "        return model\n",
        "\n",
        "    def action(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.nA) #Explore\n",
        "        action_vals = self.model.predict(state) #Exploit: Use the NN to predict the correct action from this state\n",
        "        return np.argmax(action_vals[0])\n",
        "\n",
        "    def test_action(self, state): #Exploit\n",
        "        action_vals = self.model.predict(state)\n",
        "        return np.argmax(action_vals[0])\n",
        "\n",
        "    def store(self, state, action, reward, nstate, done):\n",
        "        #Store the experience in memory\n",
        "        self.memory.append( (state, action, reward, nstate, done) )\n",
        "\n",
        "    def experience_replay(self, batch_size):\n",
        "        #Execute the experience replay\n",
        "        minibatch = random.sample( self.memory, batch_size ) #Randomly sample from memory\n",
        "\n",
        "        #Convert to numpy for speed by vectorization\n",
        "        x = []\n",
        "        y = []\n",
        "        np_array = np.array(minibatch)\n",
        "        st = np.zeros((0,self.nS)) #States\n",
        "        nst = np.zeros( (0,self.nS) )#Next States\n",
        "        for i in range(len(np_array)): #Creating the state and next state np arrays\n",
        "            st = np.append( st, np_array[i,0], axis=0)\n",
        "            nst = np.append( nst, np_array[i,3], axis=0)\n",
        "        st_predict = self.model.predict(st) #Here is the speedup! I can predict on the ENTIRE batch\n",
        "        nst_predict = self.model.predict(nst)\n",
        "        index = 0\n",
        "        for state, action, reward, nstate, done in minibatch:\n",
        "            x.append(state)\n",
        "            #Predict from state\n",
        "            nst_action_predict_model = nst_predict[index]\n",
        "            if done == True: #Terminal: Just assign reward much like {* (not done) - QB[state][action]}\n",
        "                target = reward\n",
        "            else:   #Non terminal\n",
        "                target = reward + self.gamma * np.amax(nst_action_predict_model)\n",
        "            target_f = st_predict[index]\n",
        "            target_f[action] = target\n",
        "            y.append(target_f)\n",
        "            index += 1\n",
        "        #Reshape for Keras Fit\n",
        "        x_reshape = np.array(x).reshape(batch_size,self.nS)\n",
        "        y_reshape = np.array(y)\n",
        "        epoch_count = 1 #Epochs is the number or iterations\n",
        "        hist = self.model.fit(x_reshape, y_reshape, epochs=epoch_count, verbose=0)\n",
        "        #Graph Lossesdw\n",
        "        for i in range(epoch_count):\n",
        "            self.loss.append( hist.history['loss'][i] )\n",
        "        #Decay Epsilon\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsUQAz-gnoxv",
        "outputId": "560c1804-c48e-4652-fac3-f26bb62f1174"
      },
      "source": [
        "#Create the agent\n",
        "nS = envCartPole.observation_space.shape[0] #This is only 4\n",
        "nA = envCartPole.action_space.n #Actions\n",
        "dqn = DeepQNetwork(nS, nA, learning_rate(), discount_rate(), 1, 0.001, 0.995 )\n",
        "\n",
        "batch_size = batch_size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hg6HgKhrnoxz",
        "outputId": "ba8f5186-a946-419c-bf3c-5ea75832ac61"
      },
      "source": [
        "#Training\n",
        "import copy\n",
        "rewards = [] #Store rewards for graphing\n",
        "epsilons = [] # Store the Explore/Exploit\n",
        "TEST_Episodes = 0\n",
        "counter = 0\n",
        "for e in range(EPISODES):\n",
        "    state = envCartPole.reset()\n",
        "    state = np.reshape(state, [1, nS]) # Resize to store in memory to pass to .predict\n",
        "    tot_rewards = 0\n",
        "    print(\"counter: \", counter)\n",
        "\n",
        "#BEHOLD!!! This is where we increment the \n",
        "#do i call this surreal self, surreal self adversary or...??\n",
        "    #add description\n",
        "    if counter <3: \n",
        "        self_adversary = 0 # no adversary\n",
        "    else: \n",
        "      #self_adversary = 0 # no adversary, aka equivalent to normal Q-learning\n",
        "      self_adversary = 0.01#.0001\n",
        "      self_adversary = self_adversary * counter #increment adversary\n",
        "      if self_adversary >= .33 : #.47 : #1 is fully self-destructive. Philosophically fascinating but currently useless\n",
        "        self_adversary = .33 \n",
        "      ###\n",
        "\n",
        "\n",
        "    for time in range(210): #200 is when you \"solve\" the game. This can continue forever as far as I know\n",
        "        action = dqn.action(state)\n",
        "        nstate, reward, done, _ = envCartPole.step(action)\n",
        "        re_l_rwd = copy.deepcopy(reward)\n",
        "\n",
        "        #the Surreal Adversary actually, \"Attacks\", here.\n",
        "        #fully implicit, agent is unaware of this change to its reward\n",
        "        #this is a case of Descartes Demon/Unreal Numbers. But I digress :)\n",
        "        ###***\n",
        "        reward = reward - ((reward * self_adversary)*2)\n",
        "        ###***\n",
        "        \n",
        "        nstate = np.reshape(nstate, [1, nS])\n",
        "        tot_rewards += re_l_rwd\n",
        "        dqn.store(state, action, reward, nstate, done) # Resize to store in memory to pass to .predict\n",
        "        state = nstate\n",
        "        #done: CartPole fell. \n",
        "        #time == 209: CartPole stayed upright\n",
        "        if done or time == 209:\n",
        "            rewards.append(tot_rewards)\n",
        "            epsilons.append(dqn.epsilon)\n",
        "            print(\"episode: {}/{}, score: {}, e: {}, self_adversary: {}, reward: {}, re_l_rwd: {} time: {}\"\n",
        "                  .format(e, EPISODES, tot_rewards, dqn.epsilon, self_adversary, reward, re_l_rwd, time))\n",
        "            break\n",
        "        #Experience Replay\n",
        "        if len(dqn.memory) > batch_size:\n",
        "            dqn.experience_replay(batch_size)\n",
        "    #If our current NN passes we are done\n",
        "    #I am going to use the last 5 runs\n",
        "    if len(rewards) > 5 and np.average(rewards[-5:]) > 195:\n",
        "        #Set the rest of the EPISODES for testing\n",
        "        TEST_Episodes = EPISODES - e\n",
        "        TRAIN_END = e\n",
        "        break\n",
        "    counter = counter + 1 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:51: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "counter:  0\n",
            "episode: 0/500, score: 29.0, e: 0.9801495006250001, self_adversary: 0, reward: 1.0, re_l_rwd: 1.0 time: 28\n",
            "counter:  1\n",
            "episode: 1/500, score: 15.0, e: 0.9137248860125932, self_adversary: 0, reward: 1.0, re_l_rwd: 1.0 time: 14\n",
            "counter:  2\n",
            "episode: 2/500, score: 23.0, e: 0.8183201210226743, self_adversary: 0, reward: 1.0, re_l_rwd: 1.0 time: 22\n",
            "counter:  3\n",
            "episode: 3/500, score: 29.0, e: 0.7111635524897149, self_adversary: 0.03, reward: 0.94, re_l_rwd: 1.0 time: 28\n",
            "counter:  4\n",
            "episode: 4/500, score: 13.0, e: 0.6696478204705644, self_adversary: 0.04, reward: 0.92, re_l_rwd: 1.0 time: 12\n",
            "counter:  5\n",
            "episode: 5/500, score: 18.0, e: 0.6149486215357263, self_adversary: 0.05, reward: 0.9, re_l_rwd: 1.0 time: 17\n",
            "counter:  6\n",
            "episode: 6/500, score: 13.0, e: 0.5790496471185967, self_adversary: 0.06, reward: 0.88, re_l_rwd: 1.0 time: 12\n",
            "counter:  7\n",
            "episode: 7/500, score: 9.0, e: 0.5562889678716474, self_adversary: 0.07, reward: 0.86, re_l_rwd: 1.0 time: 8\n",
            "counter:  8\n",
            "episode: 8/500, score: 11.0, e: 0.5290920728090721, self_adversary: 0.08, reward: 0.84, re_l_rwd: 1.0 time: 10\n",
            "counter:  9\n",
            "episode: 9/500, score: 14.0, e: 0.49571413690105054, self_adversary: 0.09, reward: 0.8200000000000001, re_l_rwd: 1.0 time: 13\n",
            "counter:  10\n",
            "episode: 10/500, score: 11.0, e: 0.47147873742168567, self_adversary: 0.1, reward: 0.8, re_l_rwd: 1.0 time: 10\n",
            "counter:  11\n",
            "episode: 11/500, score: 9.0, e: 0.4529463432347434, self_adversary: 0.11, reward: 0.78, re_l_rwd: 1.0 time: 8\n",
            "counter:  12\n",
            "episode: 12/500, score: 18.0, e: 0.4159480862733536, self_adversary: 0.12, reward: 0.76, re_l_rwd: 1.0 time: 17\n",
            "counter:  13\n",
            "episode: 13/500, score: 11.0, e: 0.39561243860243744, self_adversary: 0.13, reward: 0.74, re_l_rwd: 1.0 time: 10\n",
            "counter:  14\n",
            "episode: 14/500, score: 11.0, e: 0.37627099809304654, self_adversary: 0.14, reward: 0.72, re_l_rwd: 1.0 time: 10\n",
            "counter:  15\n",
            "episode: 15/500, score: 9.0, e: 0.3614809303671764, self_adversary: 0.15, reward: 0.7, re_l_rwd: 1.0 time: 8\n",
            "counter:  16\n",
            "episode: 16/500, score: 11.0, e: 0.3438081748424137, self_adversary: 0.16, reward: 0.6799999999999999, re_l_rwd: 1.0 time: 10\n",
            "counter:  17\n",
            "episode: 17/500, score: 13.0, e: 0.3237376186352221, self_adversary: 0.17, reward: 0.6599999999999999, re_l_rwd: 1.0 time: 12\n",
            "counter:  18\n",
            "episode: 18/500, score: 10.0, e: 0.30945741577570285, self_adversary: 0.18, reward: 0.64, re_l_rwd: 1.0 time: 9\n",
            "counter:  19\n",
            "episode: 19/500, score: 10.0, e: 0.29580711868545667, self_adversary: 0.19, reward: 0.62, re_l_rwd: 1.0 time: 9\n",
            "counter:  20\n",
            "episode: 20/500, score: 9.0, e: 0.28417984116121187, self_adversary: 0.2, reward: 0.6, re_l_rwd: 1.0 time: 8\n",
            "counter:  21\n",
            "episode: 21/500, score: 11.0, e: 0.2702863258025825, self_adversary: 0.21, reward: 0.5800000000000001, re_l_rwd: 1.0 time: 10\n",
            "counter:  22\n",
            "episode: 22/500, score: 9.0, e: 0.25966219297210513, self_adversary: 0.22, reward: 0.56, re_l_rwd: 1.0 time: 8\n",
            "counter:  23\n",
            "episode: 23/500, score: 11.0, e: 0.24696734223472733, self_adversary: 0.23, reward: 0.54, re_l_rwd: 1.0 time: 10\n",
            "counter:  24\n",
            "episode: 24/500, score: 8.0, e: 0.2384520680152932, self_adversary: 0.24, reward: 0.52, re_l_rwd: 1.0 time: 7\n",
            "counter:  25\n",
            "episode: 25/500, score: 10.0, e: 0.22793384675362674, self_adversary: 0.25, reward: 0.5, re_l_rwd: 1.0 time: 9\n",
            "counter:  26\n",
            "episode: 26/500, score: 10.0, e: 0.2178795886667409, self_adversary: 0.26, reward: 0.48, re_l_rwd: 1.0 time: 9\n",
            "counter:  27\n",
            "episode: 27/500, score: 12.0, e: 0.20619134658263935, self_adversary: 0.27, reward: 0.45999999999999996, re_l_rwd: 1.0 time: 11\n",
            "counter:  28\n",
            "episode: 28/500, score: 9.0, e: 0.19808659230739353, self_adversary: 0.28, reward: 0.43999999999999995, re_l_rwd: 1.0 time: 8\n",
            "counter:  29\n",
            "episode: 29/500, score: 11.0, e: 0.18840216465300522, self_adversary: 0.29, reward: 0.42000000000000004, re_l_rwd: 1.0 time: 10\n",
            "counter:  30\n",
            "episode: 30/500, score: 13.0, e: 0.17740377510930716, self_adversary: 0.3, reward: 0.4, re_l_rwd: 1.0 time: 12\n",
            "counter:  31\n",
            "episode: 31/500, score: 11.0, e: 0.16873052768933355, self_adversary: 0.31, reward: 0.38, re_l_rwd: 1.0 time: 10\n",
            "counter:  32\n",
            "episode: 32/500, score: 9.0, e: 0.16209824418995536, self_adversary: 0.32, reward: 0.36, re_l_rwd: 1.0 time: 8\n",
            "counter:  33\n",
            "episode: 33/500, score: 11.0, e: 0.15417328217978102, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 10\n",
            "counter:  34\n",
            "episode: 34/500, score: 11.0, e: 0.14663577052834775, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 10\n",
            "counter:  35\n",
            "episode: 35/500, score: 13.0, e: 0.13807558583895513, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 12\n",
            "counter:  36\n",
            "episode: 36/500, score: 11.0, e: 0.1313250884614265, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 10\n",
            "counter:  37\n",
            "episode: 37/500, score: 16.0, e: 0.12181307688414106, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 15\n",
            "counter:  38\n",
            "episode: 38/500, score: 12.0, e: 0.11527836319047392, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 11\n",
            "counter:  39\n",
            "episode: 39/500, score: 14.0, e: 0.10800599224428936, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 13\n",
            "counter:  40\n",
            "episode: 40/500, score: 16.0, e: 0.10018300685223579, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 15\n",
            "counter:  41\n",
            "episode: 41/500, score: 19.0, e: 0.09153970651645797, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 18\n",
            "counter:  42\n",
            "episode: 42/500, score: 22.0, e: 0.08239373898667031, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 21\n",
            "counter:  43\n",
            "episode: 43/500, score: 16.0, e: 0.07642587550895225, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 15\n",
            "counter:  44\n",
            "episode: 44/500, score: 9.0, e: 0.07342180695061275, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 8\n",
            "counter:  45\n",
            "episode: 45/500, score: 10.0, e: 0.07018314008827135, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 9\n",
            "counter:  46\n",
            "episode: 46/500, score: 10.0, e: 0.06708733218678724, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 9\n",
            "counter:  47\n",
            "episode: 47/500, score: 10.0, e: 0.06412808167716157, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 9\n",
            "counter:  48\n",
            "episode: 48/500, score: 10.0, e: 0.06129936495526111, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 9\n",
            "counter:  49\n",
            "episode: 49/500, score: 8.0, e: 0.05918580250061433, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 7\n",
            "counter:  50\n",
            "episode: 50/500, score: 25.0, e: 0.052477299559006776, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 24\n",
            "counter:  51\n",
            "episode: 51/500, score: 23.0, e: 0.046997986793891174, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 22\n",
            "counter:  52\n",
            "episode: 52/500, score: 12.0, e: 0.04447676004441063, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 11\n",
            "counter:  53\n",
            "episode: 53/500, score: 11.0, e: 0.04230229704853423, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 10\n",
            "counter:  54\n",
            "episode: 54/500, score: 47.0, e: 0.03359121130473201, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 46\n",
            "counter:  55\n",
            "episode: 55/500, score: 50.0, e: 0.026275840769466357, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 49\n",
            "counter:  56\n",
            "episode: 56/500, score: 91.0, e: 0.016735314893855303, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 90\n",
            "counter:  57\n",
            "episode: 57/500, score: 37.0, e: 0.013972200057807112, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 36\n",
            "counter:  58\n",
            "episode: 58/500, score: 59.0, e: 0.010447285872500434, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 58\n",
            "counter:  59\n",
            "episode: 59/500, score: 52.0, e: 0.008090597512943647, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 51\n",
            "counter:  60\n",
            "episode: 60/500, score: 43.0, e: 0.006554657266046809, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 42\n",
            "counter:  61\n",
            "episode: 61/500, score: 44.0, e: 0.005283752423423922, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 43\n",
            "counter:  62\n",
            "episode: 62/500, score: 58.0, e: 0.003970617593100295, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 57\n",
            "counter:  63\n",
            "episode: 63/500, score: 31.0, e: 0.003416256609241872, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 30\n",
            "counter:  64\n",
            "episode: 64/500, score: 33.0, e: 0.002909973732624202, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 32\n",
            "counter:  65\n",
            "episode: 65/500, score: 33.0, e: 0.0024787210368374577, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 32\n",
            "counter:  66\n",
            "episode: 66/500, score: 41.0, e: 0.002028387298846278, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 40\n",
            "counter:  67\n",
            "episode: 67/500, score: 34.0, e: 0.0017191451527812486, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 33\n",
            "counter:  68\n",
            "episode: 68/500, score: 39.0, e: 0.0014209853988328526, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 38\n",
            "counter:  69\n",
            "episode: 69/500, score: 40.0, e: 0.0011686642648686973, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 39\n",
            "counter:  70\n",
            "episode: 70/500, score: 39.0, e: 0.0009954703940636294, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 38\n",
            "counter:  71\n",
            "episode: 71/500, score: 41.0, e: 0.0009954703940636294, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 40\n",
            "counter:  72\n",
            "episode: 72/500, score: 114.0, e: 0.0009954703940636294, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 113\n",
            "counter:  73\n",
            "episode: 73/500, score: 38.0, e: 0.0009954703940636294, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 37\n",
            "counter:  74\n",
            "episode: 74/500, score: 33.0, e: 0.0009954703940636294, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 32\n",
            "counter:  75\n",
            "episode: 75/500, score: 104.0, e: 0.0009954703940636294, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 103\n",
            "counter:  76\n",
            "episode: 76/500, score: 70.0, e: 0.0009954703940636294, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 69\n",
            "counter:  77\n",
            "episode: 77/500, score: 40.0, e: 0.0009954703940636294, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 39\n",
            "counter:  78\n",
            "episode: 78/500, score: 51.0, e: 0.0009954703940636294, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 50\n",
            "counter:  79\n",
            "episode: 79/500, score: 47.0, e: 0.0009954703940636294, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 46\n",
            "counter:  80\n",
            "episode: 80/500, score: 52.0, e: 0.0009954703940636294, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 51\n",
            "counter:  81\n",
            "episode: 81/500, score: 34.0, e: 0.0009954703940636294, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 33\n",
            "counter:  82\n",
            "episode: 82/500, score: 39.0, e: 0.0009954703940636294, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 38\n",
            "counter:  83\n",
            "episode: 83/500, score: 72.0, e: 0.0009954703940636294, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 71\n",
            "counter:  84\n",
            "episode: 84/500, score: 44.0, e: 0.0009954703940636294, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 43\n",
            "counter:  85\n",
            "episode: 85/500, score: 82.0, e: 0.0009954703940636294, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 81\n",
            "counter:  86\n",
            "episode: 86/500, score: 66.0, e: 0.0009954703940636294, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 65\n",
            "counter:  87\n",
            "episode: 87/500, score: 65.0, e: 0.0009954703940636294, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 64\n",
            "counter:  88\n",
            "episode: 88/500, score: 162.0, e: 0.0009954703940636294, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 161\n",
            "counter:  89\n",
            "episode: 89/500, score: 112.0, e: 0.0009954703940636294, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 111\n",
            "counter:  90\n",
            "episode: 90/500, score: 152.0, e: 0.0009954703940636294, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 151\n",
            "counter:  91\n",
            "episode: 91/500, score: 150.0, e: 0.0009954703940636294, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 149\n",
            "counter:  92\n",
            "episode: 92/500, score: 157.0, e: 0.0009954703940636294, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 156\n",
            "counter:  93\n",
            "episode: 93/500, score: 187.0, e: 0.0009954703940636294, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 186\n",
            "counter:  94\n",
            "episode: 94/500, score: 210.0, e: 0.0009954703940636294, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 209\n",
            "counter:  95\n",
            "episode: 95/500, score: 210.0, e: 0.0009954703940636294, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 209\n",
            "counter:  96\n",
            "episode: 96/500, score: 210.0, e: 0.0009954703940636294, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 209\n",
            "counter:  97\n",
            "episode: 97/500, score: 210.0, e: 0.0009954703940636294, self_adversary: 0.33, reward: 0.33999999999999997, re_l_rwd: 1.0 time: 209\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT5Ns74-nox3"
      },
      "source": [
        "#Test the agent that was trained\n",
        "#   In this section we ALWAYS use exploit don't train any more\n",
        "for e_test in range(TEST_Episodes):\n",
        "    state = envCartPole.reset()\n",
        "    state = np.reshape(state, [1, nS])\n",
        "    tot_rewards = 0\n",
        "    for t_test in range(210):\n",
        "        action = dqn.test_action(state)\n",
        "        nstate, reward, done, _ = envCartPole.step(action)\n",
        "        nstate = np.reshape( nstate, [1, nS])\n",
        "        tot_rewards += reward\n",
        "        #DON'T STORE ANYTHING DURING TESTING\n",
        "        state = nstate\n",
        "        #done: CartPole fell. \n",
        "        #t_test == 209: CartPole stayed upright\n",
        "        if done or t_test == 209: \n",
        "            rewards.append(tot_rewards)\n",
        "            epsilons.append(0) #We are doing full exploit\n",
        "            print(\"episode: {}/{}, score: {}, e: {}\"\n",
        "                  .format(e_test, TEST_Episodes, tot_rewards, 0))\n",
        "            break;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYldJ45lnox8"
      },
      "source": [
        "**Results**  \n",
        "Here is a graph of the results. If everything was done correctly you should see the rewards over the red line.  \n",
        "\n",
        "Black: This is the 100 episode rolling average  \n",
        "Red: This is the \"solved\" line at 195  \n",
        "Blue: This is the reward for each episode  \n",
        "Green: This is the value of epsilon scaled by 200  \n",
        "Yellow: This is where the tests started."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1wgi9Hmnox8"
      },
      "source": [
        "rolling_average = np.convolve(rewards, np.ones(100)/100)\n",
        "\n",
        "plt.plot(rewards, label='Episode Reward')\n",
        "plt.plot(rolling_average, color='r', label='Rolling Avg')\n",
        "plt.axhline(y=195, color='y', linestyle='-', label='Solved Line') #Solved Line\n",
        "#Scale Epsilon (0.001 - 1.0) to match reward (0 - 200) range\n",
        "eps_graph = [200*x for x in epsilons]\n",
        "plt.plot(eps_graph, color='g',linestyle='-',label='Epsilon Decay')\n",
        "#Plot the line where TESTING begins\n",
        "#plt.axvline(x=TRAIN_END, color='o', linestyle='-')\n",
        "plt.xlim( (0,EPISODES) )\n",
        "plt.ylim( (0,220) )\n",
        "plt.legend()\n",
        "plt.title('Results for Single Surreal Agent')\n",
        "plt.xlabel('Number of Episodes')\n",
        "plt.ylabel('Score: Total Time per Episode')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "envCartPole.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prausrkR4PDl"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqI6-Z39SE5_"
      },
      "source": [
        "%cd /content/drive/My Drive/Starcraft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXrvaxt5yudW"
      },
      "source": [
        "#pickle results so we can compare results of multiple runs\n",
        "import pickle\n",
        "pickle.dump( rewards, open(\"dqn_surreal_rewards_v5.txt\", \"wb\" ) )\n",
        "pickle.dump( rolling_average, open(\"dqn_surreal_rolling_average_v5.txt\", \"wb\" ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUry-uzJywU7"
      },
      "source": [
        "#load pickled results\n",
        "loaded_rewards = pickle.load(open(\"dqn_surreal_rewards_v5.txt\", \"rb\" ))\n",
        "print(loaded_rewards)\n",
        "\n",
        "loaded_moving_average = pickle.load(open(\"dqn_surreal_rolling_average_v5.txt\", \"rb\" ))\n",
        "print(loaded_moving_average)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1K2Pd2fOnoyA"
      },
      "source": [
        "**Changes**  \n",
        "*hyper parameters*: You can alter alpha, gamma, batch size, and episode length to see what differences the algorithm returns.  \n",
        "*Training End*: You can also change the line where I only check the last 5 runs before switching to testing mode (if len(rewards) > 5 and np.average(rewards[-5:]) > 195:) as that doesn't prove it was solved. The reason I did this was because I wanted to limit the amount of runs I made.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWI2jLUSnoyA"
      },
      "source": [
        "**Conclusion**  \n",
        "This is a Deep Q-Network implementation. There are some changes you can make here and there but it follows the paper. Hopefully, you were able to understand the code as well as make your own version to compare with this one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "0YfvNUCmnoyB"
      },
      "source": [
        "**Reference**  \n",
        "Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... & Petersen, S. (2015). *Human-level control through deep reinforcement learning*. Nature, 518(7540), 529"
      ]
    }
  ]
}